[toc]

## 前言

> 学习要符合如下的标准化链条：了解概念->探究原理->深入思考->总结提炼->底层实现->延伸应用"

## 01.学习概述

- **学习主题**：
- **知识类型**：
  - [ ] ✅Android/ 
    - [ ] ✅01.基础组件与机制 
      - [ ] ✅四大组件
      - [ ] ✅IPC机制
      - [ ] ✅消息机制
      - [ ] ✅事件分发机制
      - [ ] ✅View与渲染体系（含Window、复杂控件、动画）
      - [ ] ✅存储与数据安全（SharedPreferences/DataStore/Room/Scoped Storage）
    - [ ] ✅02. 架构与工程化
      - [ ] ✅架构模式（MVC/MVP/MVVM/MVI）
      - [ ] ✅依赖注入（Koin/Hilt/Dagger）
      - [ ] ✅路由与模块化（ARouter、Navigation）
      - [ ] ✅Gradle与构建优化
      - [ ] ✅插件化与动态化
      - [ ] ✅插桩与监控框架
    - [ ] ✅03.性能优化与故障诊断
      - [ ] ✅ANR分析与优化
      - [ ] ✅启动耗时优化
      - [ ] ✅内存泄漏监控
      - [ ] ✅监控与诊断工具
    - [ ] ✅04.Jetpack与生态框架
      - [ ] ✅Room
      - [ ] ✅Paging
      - [ ] ✅WorkManager
      - [ ] ✅Compose
    - [ ] ✅05.Framework与系统机制
      - [ ] ✅ActivityManagerService (含ANR触发机制)
      - [ ] ✅Binder机制
  - [ ] ✅音视频开发/
    - [x] ✅01.基础知识
    - [ ] ✅02.OpenGL渲染视频
    - [ ] ✅03.FFmpeg音视频解码
  - [ ] ✅ Java/
    - [ ] ✅01.基础知识···
    - [ ] ✅02.集合框架
    - [ ] ✅03.异常处理
    - [ ] ✅04.多线程与并发
    - [ ] ✅06.JVM
  - [ ] ✅ Kotlin/
    - [ ] ✅01.基础语法
    - [ ] ✅02.高阶扩展
    - [ ] ✅03.协程和流
  - [ ] ✅ Flutter/
    - [ ] ✅01.基础基础语法
    - [ ] ✅02.状态管理
    - [ ] ✅03.路由与依赖注入
    - [ ] ✅04.原生通信
  - [ ] ✅ 自我管理/
    - [ ] ✅01.内观
  - [ ] ✅ 项目经验/
    - [ ] ✅01.启动逻辑
    - [ ] ✅02.云值守
    - [ ] ✅03.智控平台
- **学习来源**：
- **重要程度**：⭐⭐⭐⭐⭐
- **学习日期**：2025.
- **记录人**：@panruiqi

### 1.1 学习目标

- 了解概念->探究原理->深入思考->总结提炼->底层实现->延伸应用"

### 1.2 前置知识

- [ ] 

## 02.核心概念

### 2.1 是什么？




### 2.2 解决什么问题？

音视频解码的流程

### 2.3 基本特性



## 03.原理机制

### 3.1 什么是音视频解码

首先要看一些基础概念

- H264 和 YUV
  - H264是一种视频压缩编码格式，它的本质是“把一连串YUV帧用复杂算法压缩成很小的数据流”，他有帧内压缩算法，帧间压缩算法。
  - H264码流体积小，适合存储和传输，但不能直接显示，必须解码还原成YUV帧
  - YUV 是一种色彩空间，一帧YUV数据就是一张未压缩的画面。

  > H264如何压缩YUV帧？分为帧内压缩和帧间压缩
  >
  > - 帧内压缩：只压缩当前帧内部的冗余信息，不考虑前后帧。
  >
  >   - I帧（关键帧）就是用帧内压缩算法生成的。
  >
  >   - 原理：类似于JPEG图片压缩，把一张图片分块、变换、量化、编码，去除空间上的冗余（比如一片蓝天，很多像素颜色很接近）
  >
  > - 帧间压缩：利用前后帧之间的相似性，只记录变化的部分。
  >
  >   - P帧/B帧就是用帧间压缩算法生成的
  >   - 帧间压缩只记录“和前一帧/后一帧的差异”，不重复存储相同的内容，典型算法：帧差法（Frame Differencing），只保存像素的变化量
  >
  > - 所以，解压缩意思就是：
  >
  >   - 遇到I帧：I帧是完整帧，解码器可以直接把I帧解压缩还原成一张完整的YUV图像（不依赖其他帧）
  >   - 遇到P帧/B帧：先找到最近的I帧（或参考帧）的解压缩后的YUV数据，然后根据P/B帧的差异信息，叠加/修正，生成当前帧的YUV图像

- MediaCodec 

  - 他是 Android 提供的编解码 API，它可以调用硬件加速（硬解码/硬编码），也可以在某些情况下用软件解码。你可以用它来解码视频（如H264、H265）和音频（如AAC），也可以用来编码。
  - MediaCodec的解码过程简述就是：
    - 你（Client）提供H264码流
    - MediaCodec（Codec）解码H264码流，还原成一帧帧YUV数据
    - 你（Client）拿到YUV帧，可以用来直接送给显示模块（如SurfaceView、OpenGL等）渲染、后处理、保存等。
- 同理，音频AAC解码后得到PCM数据
  
  > 那我有一个疑问了，如何抽帧做成显示的封面？
  >
  > 本质是抽出YUV，然后调用API转换为RGB，最终显示为bitmap。图片基本是RGB的样式，看前面，有YUV到RGB的转换

好，接着来看MediaCodec进行解码的数据流

- 整体如下：
  - <img src="../../_pic_/image-20250921160650647.png" alt="image-20250921160650647" style="zoom:80%;" />
- 图中元素说明：
  - Client（左侧）：你的App代码，负责提供原始数据（比如H264码流、PCM音频等），
  - Codec（中间蓝色块）：MediaCodec硬件/软件编解码器，负责实际的编码/解码工作，
  - Client（右侧）：你的App代码，负责消费解码后的数据（比如YUV帧、AAC帧等）。
- input（输入缓冲区）空的input buffer：MediaCodec分配好一批空缓冲区，等你把待解码/编码的数据填进去。
- output（输出缓冲区）：MediaCodec处理完后，把解码/编码好的数据放到输出缓冲区，等你来取。你用完数据后，必须“释放”缓冲区，否则MediaCodec没法继续输出
- MediaCodec的核心就是“缓冲区队列”机制，你和解码器通过input/output buffer交互数据，通过缓冲区队列，你和MediaCodec各自处理自己的部分，互不阻塞，提高效率。

接着来看看MediaCodec进行解码时的状态。

- 整体如下：

  - <img src="../../_pic_/image-20250921160800345.png" alt="image-20250921160800345" style="zoom:80%;" />

- 我们可以看到有三种状态

- Stopped（停止态）

  - Uninitialized：刚创建MediaCodec对象，还没配置参数。

  - Configured：调用configure()后，参数已设置好，但还没开始解码/编码。

  - Error：出错状态，比如参数错误、解码器崩溃等

-  Executing（执行态）

  - Flushed：调用start()后，解码器已准备好，输入/输出缓冲区已清空，等待数据输入。

  - Running：正在解码/编码，数据在流动。你在不断送入数据、取出结果。

  - End of Stream：输入流结束（你送了EOS标志），解码器处理完所有数据，进入流结束状态，此时你可以安全地关闭解码器或重新开始

-  Released（已释放）

  - 调用release()后，所有资源释放，MediaCodec对象不可再用

- 状态流转过程

  - 创建对象：Uninitialized
  - 配置参数：configure() → Configured
  - 开始解码/编码：start() → Flushed
  - 输入数据：dequeueInputBuffer/queueInputBuffer → Running
  - 输入结束：queueInputBuffer(EOS) → End of Stream
  - 重新开始/跳播：flush() → Flushed
  - 停止/重置：stop()/reset() → Uninitialized
  - 出错：Error
  - 释放资源：release() → Released

### 3.2 解码流程

好，我们看到了他的数据流向和状态图。接下来看看他的解码流程。

- 整体如下图
  - <img src="../../_pic_/image-20250921161241099.png" alt="image-20250921161241099" style="zoom:67%;" />
- 分为三个主要的阶段：初始化阶段，解码循环阶段，数据流结束阶段
- 初始化阶段
  - 新建Codec：MediaCodec.createDecoderByType()，创建解码器实例，指定解码类型（如"video/avc"表示H264）
  - 配置Codec：configure()，设置解码参数（如宽高、帧率、输出Surface等）
  - 开始Codec，start()，进入flushed可用状态，准备接收数据
- 解码循环阶段，我们这里以同步模式为例
  - 输入数据：调用dequeueInputBuffer()获取一个空的输入缓冲区index。把待解码的H264（或AAC等）数据写入该缓冲区。用queueInputBuffer()把缓冲区送进解码器
  - MediaCodec处理：这阶段不用我们考虑，由MediaCodec调用底层硬件解码，根据我们配置的解码参数，输出对应的YUV到相关的输出缓冲区中
  - 获取输出数据：调用dequeueOutputBuffer()获取解码后的输出缓冲区index。如果有数据（index >= 0），说明有一帧解码完成。
  - 数据有效性验证：如果有有效数据（Y），进入渲染流程。如果没有有效数据（N），继续循环等待
  - 渲染：将解码后的数据（如YUV帧）渲染到Surface、OpenGL或转成Bitmap显示
  - 释放输出缓冲区：用完输出缓冲区后，必须调用releaseOutputBuffer()，否则解码器会卡住。
  - ok，上面是解码循环的阶段，我们输入数据，MediaCodec自动处理，获取输出数据，渲染，然后释放已使用的输出缓冲区。
- 结束阶段
  -  判断是否结束（End of Stream）：如果检测到输入流结束（EOS），则退出循环，进入结束流程
  - 调用release()释放解码器资源，流程结束。

### 3.3 封装基础解码框架_解码器参数部分

根据上面的流程图，可以发现，无论音频还是视频，解码流程基本是一致的，不同的地方只在于【配置】、【渲染】两个部分。

好，我们已经知道解码的基础原理了。接下来我们来自己封装一个基础解码框架

定义解码器，我们分为参数部分和解码流程两个部分讲解。

首先是解码器参数部分

- 我们将整个解码流程抽象为一个解码基类：BaseDecoder。为了规范代码和更好的拓展性，我们先定义一个解码器：IDecoder，继承Runnable。其定义了解码器的所有核心操作（暂停、继续、停止、状态获取、参数获取等），这样你可以很容易地实现不同类型的解码器（如视频、音频、软解、硬解等）。

- IDecoder：好，我们来看一下他的代码

  - 首先是解码流程控制
  - <img src="../../_pic_/image-20250921162643674.png" alt="image-20250921162643674" style="zoom:67%;" />
  - 然后是解码状态查询和监听，setStateListener(l: IDecoderStateListener?)：设置状态监听器，便于UI层或上层感知解码进度、错误、完成等事件
  - <img src="../../_pic_/image-20250921162732077.png" alt="image-20250921162732077" style="zoom:67%;" />
  - 最后是媒体信息获取
  - ![image-20250921162814882](../../_pic_/image-20250921162814882.png)

  > 为什么继承自Runnable？
  >
  > - 解码是耗时操作，需要不断循环处理数据（同步模式下），如果放在主线程会卡UI。
  >
  > - 继承Runnable后，可以把解码任务丢到线程池或新线程中，异步执行，不会阻塞主线程。
  >
  > - 这样也便于后续支持“暂停/恢复/停止”等线程控制操作。

- 接着，继承IDecoder，定义基础解码器BaseDecoder，这里定义他的基本参数，方法定义暂时省略

  - 首先是线程相关
    - mIsRunning：标记解码线程是否应该继续运行，用于控制解码循环的退出，比如用户点击“停止”时设为false，线程安全退出。
    - mLock：线程同步锁，用于实现“暂停/恢复”功能，挂起/唤醒解码线程，保证线程安全
    - mReadyForDecode：标记是否可以进入解码主循环。用于在暂停、等待数据等场景下，线程可以wait，直到条件满足再继续
    - ![image-20250921163537482](../../_pic_/image-20250921163537482.png)
  - 然后是解码相关成员
    - mCodec: MediaCodec?，Android原生的音视频编解码器实例
    - mExtractor: IExtractor?,自定义的音视频数据读取器接口，解耦数据源和解码器，支持多种数据来源（如本地文件、网络流、内存等）
    - mInputBuffers: Array<ByteBuffer>? / mOutputBuffers: Array<ByteBuffer>?：解码器的输入/输出缓冲区数组（同步模式下用），存放待解码的数据和解码后的数据帧
    - mBufferInfo: MediaCodec.BufferInfo：存储解码帧的元数据（如时间戳、flags等），用于判断帧类型、同步音视频、处理EOS等
    - mState: DecodeState：当前解码器的状态（如STOP、DECODING、PAUSE等），便于状态管理和外部监听
    - mStateListener: IDecoderStateListener?：解码状态监听器，让UI或上层感知解码进度、错误、完成等事件
    - mIsEOS：标记流数据是否结束（End of Stream），用于控制解码循环的退出和资源释放
    - mVideoWidth / mVideoHeight：视频的宽高，便于渲染、UI展示、后续处理
    - ![image-20250921163758074](../../_pic_/image-20250921163758074.png)

  > 先是IDecoder，然后是BaseDecoder，他是怎么设计他的解码流程的？
  >
  > - IDecoder：定义了解码器的“规范”（接口），规定了解码器必须有哪些功能（如暂停、继续、停止、获取宽高等），保证所有解码器有统一的“外壳”和调用方式
  > - BaseDecoder：需要实现IDecoder接口，并封装解码的通用的、重复的逻辑（比如线程管理、状态管理、MediaCodec、Extractor等，缓冲区管理等）都写好，子类不用重复造轮子，但它是abstract的，不能直接用来解码
  > - 接着我们要实现具体的解码器：比如VideoDecoder、AudioDecoder等，继承自BaseDecoder，实现具体的解码细节（比如视频帧的渲染、音频的播放、数据的特殊处理等）
  > - 举例：
  > - ![image-20250921164220654](../../_pic_/image-20250921164220654.png)

- 解码状态

  - ![image-20250921164259830](../../_pic_/image-20250921164259830.png)

- 定义音视频数据分离器

  - 前面说过，MediaCodec需要我们不断地喂数据给输入缓冲，那么数据从哪里来呢？肯定是音视频文件了，这里的IExtractor就是自定义的音视频数据读取器接口，解耦数据源和解码器，支持多种数据来源（如本地文件、网络流、内存等）
  - getFormat(): MediaFormat?：获取当前音视频流的格式参数（如采样率、分辨率、编码类型等），解码器需要这些参数来正确配置MediaCodec
  - readBuffer(byteBuffer: ByteBuffer): Int：从数据源读取一帧或一段音视频数据，写入byteBuffer，返回实际读取的字节数
  - getCurrentTimestamp(): Long：获取当前帧的时间戳（单位：微秒），用于音视频同步、进度显示、seek等。
  - seek(pos: Long): Long：跳转到指定位置（单位：微秒），返回实际跳转到的帧的时间戳。支持快进、快退、拖动进度条等功能
  - setStartPos(pos: Long)：设置数据读取的起始位置，便于分段播放、断点续播等场景。
  - stop()：停止数据读取，释放资源。便于解码器安全退出，防止内存泄漏
  - ![image-20250921164701611](../../_pic_/image-20250921164701611.png)

  > IExtractor 是我们自定义的一个抽象接口，规定了数据读取器应该有哪些功能（如readBuffer、seek、getFormat等）。
  >
  > 但是真正的读取能力，依赖于MediaExtractor 这个 Android SDK自带的音视频数据读取类，其可以从本地文件、assets、网络等多种数据源中提取音视频轨道和帧数据，**并自动解析常见的多媒体容器格式（如MP4、MKV、3GP等），并能分离出音频轨、视频轨等**
  >
  > 那么这是一个实现实例：
  >
  > - ![image-20250921165039961](../../_pic_/image-20250921165039961.png)
  > - 这样解码器只依赖IExtractor接口，不关心底层用的是什么实现。他也不需要关系是真正的读取能力是谁提供的，只要readBuffer在具体类下，调用具体的MediaExtractor 实现即可，他只需要管理IExtractor接口。

  > 我现在好奇一点：快进怎么处理？他的本质是什么？因为我们正常是一帧一帧处理的，快进呢？
  >
  > - 快进不是“加快速度一帧一帧播放”，而是直接跳到目标时间点附近的关键帧（I帧/GOP起始），然后从那里开始解码和播放。
  > - 那么GOP该选哪一个呢？播放器会根据快进速率和目标时间戳，计算应该seek到哪个GOP的I帧

### 3.4 封装基础解码框架_解码器流程之run方法

好了，我们有解码器参数部分的定义了，接下来是解码流程的处理。

还记得我们的解码流程吗？回忆一下，初始化阶段，解码循环阶段，结束阶段。

- <img src="../../_pic_/image-20250921161241099.png" alt="image-20250921161241099" style="zoom:67%;" />

好，我们继续，我们的IDecoder继承了Runnable，在Runnable中 有run回调方法，这个run回调方法中集成了整个解码流程:

首先是初始化阶段

- 设置初始状态，通知监听器“解码准备”，调用init()做解码器和数据源的初始化（如MediaCodec、Extractor等），init我们稍后再说，先看流程。
  - ![image-20250922113838932](../../_pic_/image-20250922113838932.png)

然后是解码主循环

- 具体如下：

  - 前置判断：
    - 非START、DECODING、SEEKING这些执行状态则调用waitDecode挂起
    - 停止状态则中断循环
  - 输入数据：如果还没到流结尾（EOS），从Extractor读取数据，送入解码器输入缓冲区，返回值标记是否已经到达流结尾。
  - 处理数据：这时，MediaCodec会自动处理输入缓冲区，我们从输出缓冲区中读取就行，如果输出缓冲区没数据，我们会被阻塞。当MediaCodec处理完成，输出到输出缓冲区中，我们会被唤醒，从中取出数据。
  - 输出数据：
    - 从解码器输出缓冲区拉取解码好的数据
    - 渲染数据：如果有数据，调用render()进行渲染（抽象方法，子类实现），然后释放输出缓冲区
    - 如果刚开始解码，解完一帧后自动进入PAUSE状态（可选设计，便于单步调试或首帧渲染）

  - ![image-20250922120341292](../../_pic_/image-20250922120341292.png)

最后是结束阶段

- 具体如下：
  - 解码完成判断：如果输出缓冲区的flag标记为BUFFER_FLAG_END_OF_STREAM，说明解码完成，进入FINISH状态，通知监听器，同时跳出循环。
  - 资源释放：跳出循环后，调用doneDecode()（抽象方法，子类实现），最后释放解码器资源
  - ![image-20250922120816025](../../_pic_/image-20250922120816025.png)

### 3.5 封装基础解码框架_解码器流程之init方法

这是解码流程的初始化阶段

首先是初始化数据提取器

- 先是检查数据源：我们可以看到：检查mFilePath是否为空，防止后续找不到数据源
  
- ![image-20250922132223231](../../_pic_/image-20250922132223231.png)
  
- 然后是初始化数据提取器和视频源参数：

  - 调用initExtractor(mFilePath)，传递数据源路径，让子类决定用哪种Extractor（如本地文件、网络流、内存流等）
  - 通过Extractor获取MediaFormat，提取时长、宽高等通用参数。
  - 调用initSpecParams(format)，让子类提取自己特有的参数（如音频采样率、声道数，视频旋转角度等）
  - ![image-20250922132508721](../../_pic_/image-20250922132508721.png)
  - ![image-20250922132547322](../../_pic_/image-20250922132547322.png)

  > 我好奇的是：Extractor如何知道视频的信息的？比如：我们是网络数据源，他如何获取到视频信息？先下载下来？
  >
  > 本质：Extractor只需读取数据源的头部（本地或网络），就能解析出视频的元数据。
  >
  > - 对于本地文件：本地文件（如MP4、MKV等），Extractor（如MediaExtractor）可以直接打开文件，读取文件头（Header）和元数据区，这些头部信息里包含了时长、宽高、码率、轨道信息、编码类型等，Extractor解析后就能提供MediaFormat等参数。
  > - 对于网络文件：Extractor会先请求网络流的前面一小段数据（通常是文件头和部分元数据），只要拿到足够的头部信息，就能解析出视频的基本参数。

好，接着是我们的渲染器的初始化

- 调用initRender()（抽象方法），让子类初始化自己的渲染器（如视频Surface、音频AudioTrack）
  - ![image-20250922132655362](../../_pic_/image-20250922132655362.png)

最后是我们的解码器MediaCodec的初始化

- 具体如下：
  - 通过Extractor获取MediaFormat中的MIME类型（如"video/avc"、"audio/mp4a-latm"），也就是音视频编码格式，根据格式创建对应的MediaCodec实例
  - 调用configCodec(codec, format)，，让子类配置解码器（如设置Surface、加密信息等）。
  - 启动解码器mCodec!!.start()。
  - 获取输入/输出缓冲区。
  - ![image-20250922132827645](../../_pic_/image-20250922132827645.png)
  - ![image-20250922132901845](../../_pic_/image-20250922132901845.png)

### 3.6 封装基础解码框架_解码器流程之压入数据到输入缓冲

首先是输入缓冲的分配

- 我们创建MediaCodec的时候，系统会自动为该解码器分配一组输入缓冲区，也就是Buffers缓冲区数组，我们再上面init阶段获取到了其输入缓冲区的引用
  - ![image-20250922134112378](../../_pic_/image-20250922134112378.png)
- 但是，MediaCodec输入缓冲区是一个Buffers数组，我们申请空缓冲区是从这个Buffers数组中获取一个Buffer。
  - 通过dequeueInputBuffer向MediaCodec申请一个空的输入缓冲区，这个方法会阻塞最多2000ms，直到有空缓冲区可用。
  - 其返回的Index的值表示在mCodec?.inputBuffers的下标，我们可以通过mInputBuffers!![inputBufferIndex]在mInputBuffers数组中取出对应的ByteBuffer。这个ByteBuffer就是你可以写入数据的空间。
  - ![image-20250922134228951](../../_pic_/image-20250922134228951.png)

然后是写入数据到输入缓冲

- 具体如下：
  - 用Extractor从数据源读取一帧（或一段）音视频数据，sampleSize为实际读取的数据字节数。
  - 如果sampleSize<0，说明数据已经读完，那么我们压入一个空buffer，并设置BUFFER_FLAG_END_OF_STREAM标志。解码器收到这个标志后，会在输出端返回EOS，整个解码流程结束
  - 否则我们把刚才填充好的数据送进解码器，附带当前帧的时间戳。解码器会异步处理这些数据，解码出YUV帧或PCM帧。
  - ![image-20250922134946461](../../_pic_/image-20250922134946461.png)

返回是否结束

- 告诉主循环“数据是否已经全部送完”，便于后续流程判断解码是否结束。
  - ![image-20250922135115332](../../_pic_/image-20250922135115332.png)

### 3.7 封装基础解码框架_解码器流程之从输出缓冲中获取数据

- 具体如下：
  - 获取Buffers的index，同理上面的。1000：最多等待1000ms（1秒），如果填-1则无限等待
  - 不过这里有：mBufferInfo：用于接收输出帧的元数据（如时间戳、flags、大小等）
  - 判断index类型：
    - MediaCodec.INFO_OUTPUT_FORMAT_CHANGED：输出格式发生变化（如分辨率、颜色格式等）输出格式发生变化（如分辨率、颜色格式等）
    - MediaCodec.INFO_OUTPUT_BUFFERS_CHANGED：输出缓冲区数组发生变化，需要重新获取mOutputBuffers = mCodec!!.outputBuffers
    - MediaCodec.INFO_TRY_AGAIN_LATER：当前没有可用的输出数据，等会再来，本次不处理，返回-1，主循环下次再试。
    - index >= 0，有可用的解码数据，index为输出缓冲区buffer的下标。返回index，主循环会用这个index去取出解码好的数据（如YUV帧、PCM帧），进行渲染或播放
  - ![image-20250922140236062](../../_pic_/image-20250922140236062.png)

### 3.8 封装基础解码框架_解码器流程之剩余流程

渲染：

- 将解码好的数据（如YUV帧、PCM帧）交给子类处理，通常是把YUV帧渲染到Surface、OpenGL等；把PCM数据写入AudioTrack播放；
  - ![image-20250922140712597](../../_pic_/image-20250922140712597.png)

释放输出缓冲区

- 通知MediaCodec“我已经用完这个输出缓冲区了，可以回收再利用”，第二个参数表示是否立即渲染。
  - ![image-20250922140959948](../../_pic_/image-20250922140959948.png)

判断解码是否完成

- 看见下面的输入的操作吗？数据取完压入数据结束标志；当你在输入端压入EOS标志后，MediaCodec会在最后一帧输出时也带上这个标志，表示解码流程彻底结束，此时进入FINISH状态，通知监听器“解码完成”。
  - ![image-20250922141049728](../../_pic_/image-20250922141049728.png)
  - ![image-20250922134946461](../../_pic_/image-20250922134946461.png)

然后是release

- 安全释放所有解码相关资源，防止内存泄漏和资源占用
  - ![image-20250922141215155](../../_pic_/image-20250922141215155.png)

## 04.底层原理



## 05.深度思考

### 5.1 关键问题探究



### 5.2 设计对比



## 06.实践验证

### 6.1 行为验证代码



### 6.2 性能测试





## 07.应用场景

### 7.1 最佳实践



### 7.2 使用禁忌





## 08.总结提炼

### 8.1 核心收获



### 8.2 知识图谱



### 8.3 延伸思考





## 09.参考资料

1. []()
2. []()
3. []()

## 其他介绍

### 01.关于我的博客

- csdn：http://my.csdn.net/qq_35829566

- 掘金：https://juejin.im/user/499639464759898

- github：https://github.com/jjjjjjava

- 邮箱：[934137388@qq.com]

