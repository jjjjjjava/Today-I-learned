[toc]

## 前言

> 学习要符合如下的标准化链条：了解概念->探究原理->深入思考->总结提炼->底层实现->延伸应用"

## 01.学习概述

- **学习主题**：音视频基础
- **知识类型**：
  - [ ] **知识类型**：
    - [ ] ✅Android/ 
      - [ ] ✅01.基础组件
      - [ ] ✅02.IPC机制
      - [ ] ✅03.消息机制
      - [ ] ✅04.View原理
      - [ ] ✅05.事件分发机制
      - [ ] ✅06.Window
      - [ ] ✅07.复杂控件
      - [ ] ✅08.性能优化
      - [ ] ✅09.流行框架
      - [ ] ✅10.数据处理
      - [ ] ✅11.动画
      - [ ] ✅12.Groovy
    - [ ] ✅音视频开发/
      - [x] ✅01.基础知识
      - [ ] ✅02.OpenGL渲染视频
      - [ ] ✅03.FFmpeg音视频解码
    - [ ] ✅ Java/
      - [ ] ✅01.基础知识
      - [ ] ✅02.Java设计思想
      - [ ] ✅03.集合框架
      - [ ] ✅04.异常处理
      - [ ] ✅05.多线程与并发编程
      - [ ] ✅06.JVM
    - [ ] ✅ Kotlin/
      - [ ] ✅01.基础语法
      - [ ] ✅02.高阶扩展
      - [ ] ✅03.协程和流
- **学习来源**：
- **重要程度**：⭐⭐⭐⭐⭐（核心基础）  
- **学习日期**：
- **记录人**：@panruiqi

### 1.1 学习目标

- 了解概念->探究原理->深入思考->总结提炼->底层实现->延伸应用"

### 1.2 前置知识

- [ ] Kotlin基础语法

## 02.核心概念

### 2.1 是什么？

音视频基础是指我们从零开始自己实现音视频的播放流程

### 2.2 解决什么问题？

了解和掌握音视频的基础流程

### 2.3 基本特性



## 03.原理机制

### 3.1 进一步思考

好，我们从零开始自己实现音视频的播放流程是什么样的？

没有MediaPlayer时，手动进行音视频播放的流程

- 读取音视频数据源

  - 自己打开本地文件、网络流、资源文件等，读取原始数据。

- 解析音视频文件格式

  - 需要自己解析 MP3、AAC、MP4、WAV、AVI 等文件格式，提取音视频流信息（如采样率、码率、帧率、分辨率等）。

- 解码

  - 需要自己实现或集成音频/视频解码器（如 libmp3lame、libaac、FFmpeg、OpenMAX、MediaCodec 等），把压缩的音视频数据解码成原始 PCM（音频）或 YUV/RGB（视频）数据。

- 音频输出

  - 需要自己用底层 API（如 AudioTrack）把解码后的 PCM 数据送到音频硬件播放。

  - 还要处理音量、音频焦点、音频通道等问题。

- 视频渲染

  - 需要自己用 SurfaceView、OpenGL 或 Canvas 把解码后的帧数据渲染到屏幕上。

  - 还要处理帧同步、缩放、旋转等。

- 播放控制

  - 需要自己实现播放、暂停、停止、快进、快退、循环等控制逻辑。

  - 还要处理播放进度、缓冲、错误、播放完成等事件。

- 资源管理

  - 需要自己管理解码器、音视频缓冲区、线程、硬件资源，避免内存泄漏和资源冲突。

- 伪代码如下：

  - ```
    // 1. 读取音频文件
    val inputStream = FileInputStream("xxx.mp3")
    
    // 2. 解码MP3为PCM（需要用第三方库如FFmpeg或libmp3lame）
    // 3. 创建AudioTrack
    val audioTrack = AudioTrack(...)
    
    // 4. 不断读取解码后的PCM数据，写入AudioTrack
    while (有数据) {
        val pcmData = 解码一帧()
        audioTrack.write(pcmData, 0, pcmData.size)
    }
    
    // 5. 释放资源
    audioTrack.release()
    inputStream.close()
    ```

- 需要的底层API和第三方库如下：

  - 音频播放：AudioTrack（Android底层音频输出类）

  - 音频录制：AudioRecord

  - 视频渲染：SurfaceView、TextureView、OpenGL ES

  - 解码库：MediaCodec（Android原生解码器）、FFmpeg（第三方强大解码库）

  - 文件/流处理：自己实现数据读取、缓冲、同步

### 3.2 进一步思考

我没搞懂这个

- 文件格式，音视频流信息是什么
- 把压缩的音视频数据解码成原始 PCM（音频）或 YUV/RGB（视频）数据又是什么
- 需要自己用底层 API（如 AudioTrack）把解码后的 PCM 数据送到音频硬件播放。
- 需要自己用 SurfaceView、OpenGL 或 Canvas 把解码后的帧数据渲染到屏幕上

文件格式

- 指的是音频/视频文件的封装格式，比如：

- 音频：MP3、WAV、AAC、OGG

- 视频：MP4、AVI、MKV、MOV

- 一个音视频文件通常包含文件头（描述信息）+ 数据流（音频流、视频流）。

音视频流信息，也就是文件头中的描述信息

- 指的是文件里每一段音频/视频数据的参数，比如：

- 音频流：采样率、声道数、比特率、编码格式（如AAC、MP3）

- 视频流：分辨率、帧率、比特率、编码格式（如H.264、MPEG4）
- 举例：一个 MP4 文件，可能包含一条 H.264 编码的视频流和一条 AAC 编码的音频流。

把压缩的音视频数据解码成原始 PCM（音频）或 YUV/RGB（视频）数据

- 压缩数据

  - 音视频文件里的数据通常是压缩过的，比如 MP3、AAC、H.264 等。

  - 压缩的好处是体积小，便于存储和传输，但不能直接播放或显示

- 解码

  - 解码就是把压缩的数据还原成“原始数据”。

  - 音频解码后得到PCM数据，视频解码后得到YUV或RGB帧数据

- PCM（音视频原始数据）

  - Pulse Code Modulation，脉冲编码调制

  - 是最原始的音频采样数据，音频硬件只能播放PCM数据

- YUV/RGB（视频原始帧）

  - YUV：常见的视频帧格式，分亮度（Y）和色度（U/V）分量

  - RGB：每个像素有红绿蓝三色分量，适合直接显示在屏幕上

用AudioTrack 播放 PCM 数据、

- AudioTrack 是 Android 的底层音频播放类。

- 你需要把解码后的 PCM 数据一块一块写入 AudioTrack，它会把数据送到音频硬件（扬声器/耳机）播放。

- ```
  val audioTrack = AudioTrack(...)
  audioTrack.play()
  while (有PCM数据) {
      audioTrack.write(pcmBuffer, 0, pcmBuffer.size)
  }
  audioTrack.stop()
  audioTrack.release()
  ```

用 SurfaceView/OpenGL/Canvas 渲染视频帧

- 视频解码后得到一帧一帧的 YUV/RGB 数据。

- 你需要用底层的图形API把这些帧一帧一帧地画到屏幕上。

- SurfaceView：Android 提供的专门用于高效绘制视频帧的控件。

- OpenGL：更底层的图形API，可以高效渲染YUV/RGB帧，适合做特效、滤镜等。

- Canvas：Android 的2D绘图API，适合简单的图片/帧渲染。

- 伪代码：

  - ```
    while (有视频帧) {
        val frame = 解码一帧()
        surfaceView.draw(frame) // 或用OpenGL渲染
    }
    ```

整体流程如下：

- ```
  [音视频文件]
      |
  [读取文件头，解析流信息]
      |
  [解码器]
      |                |
  [PCM音频数据]   [YUV/RGB视频帧]
      |                |
  [AudioTrack]    [SurfaceView/OpenGL/Canvas]
      |                |
  [扬声器/耳机]   [屏幕显示]
  ```

  

### 3.3 进一步思考

好复杂，先别慌，我们从视频开始看，我们先不考虑视频的音频。

视频文件格式

- MP4 是一种容器格式，它定义了如何把视频流、音频流、字幕等数据组织在一个文件里。

- 文件结构：通常是文件头（Header）+ 数据流（音视频帧等）。

- 不同格式的差别：主要体现在文件头的结构、支持的流类型、元数据的组织方式，以及默认支持的压缩编码格式。

文件头和描述信息

- 文件头里包含了视频的各种参数信息，比如：

- 分辨率：视频每一帧的宽度和高度（如 1920x1080，单位是像素），决定了画面的清晰度。

- 帧率（Frame Rate）：每秒钟显示多少帧（如 30fps），影响画面流畅度。

- 比特率（Bitrate）：每秒钟传输的数据量（如 2Mbps），影响视频的清晰度和文件大小。

- 编码格式（Codec）：视频流采用的压缩算法（如 H.264、H.265、VP9），音频流的编码格式（如 AAC、MP3）。

- 时长、关键帧信息、音视频同步信息等。

编码格式和压缩

- 编码格式就是压缩格式，比如 H.264、H.265、MPEG-4、VP8、VP9 等。

- 视频文件里的数据流通常是压缩过的，不能直接显示，需要解码。

解码过程

- 解码器（如 MediaCodec、FFmpeg）会把压缩的视频流还原成一帧一帧的原始图像数据。

- 这些原始帧通常是YUV 或 RGB 格式的像素数据。

渲染流程

- 解码后得到的每一帧原始图像数据，需要送到图形系统进行显示。

- 你可以用 SurfaceView、TextureView、OpenGL 等 API，把这些帧“推”到显示队列。

- 图形系统会自动把这些帧按顺序显示在屏幕上，实现视频播放的效果。

视频播放的本质就是：解析文件头拿到参数 → 解码压缩流得到原始帧 → 用图形API把帧显示出来。

### 3.4 进一步思考

为什么需要音视频流信息？我们为什么是先解码这个格式，然后再解码呢？

为什么需要音视频流信息

- 一个音视频文件里，可能有多条流

  - 一个 MP4/MKV/AVI 文件，不只是简单的音频或视频数据，而是一个容器，里面可以有：

  - 一条或多条视频流（不同角度、不同清晰度）

  - 一条或多条音频流（不同语言、不同声道）

  - 字幕流、元数据流等

- 流信息描述了每条流的参数

  - 流信息包括：编码格式、分辨率、帧率、采样率、声道数、比特率、时长、起止时间、关键帧位置等。

  - 只有知道这些信息，播放器/解码器才能正确地选择、解码和同步音视频内容。

-  举例

  - 你要播放一个电影，里面有中英文音轨、多个字幕、主视频和花絮视频。你需要先知道每条流的类型和参数，才能让用户选择、正确解码和播放。

为什么是先解封装格式，然后解码？

- 解封装格式

  - 解封装就是解析文件头和结构，把容器里的各条流“拆”出来。

  - 例如：MP4 文件头告诉你有几条流、每条流的类型、编码格式、数据在文件中的位置等。

  - 只有解封装后，才能知道哪些数据属于视频流，哪些属于音频流，以及它们的参数。

- 解码

  - 解码是针对每条流的数据，把压缩的音视频数据还原成原始帧（如 PCM、YUV/RGB）。

  - 不同流可能用不同的编码格式（如视频用 H.264，音频用 AAC），需要用对应的解码器

为何不能一步到位？

- 因为容器格式和编码格式是两层不同的协议：

- 容器格式（如 MP4、MKV）负责“打包/组织”各种流

- 编码格式（如 H.264、AAC）负责“压缩/还原”具体的音视频数据

- 你必须先拆包（解封装），才能拿到每条流的数据，再用合适的解码器解码。

## 04.底层原理



## 05.深度思考

### 5.1 关键问题探究



### 5.2 设计对比



## 06.实践验证

### 6.1 行为验证代码



### 6.2 性能测试





## 07.应用场景

### 7.1 最佳实践



### 7.2 使用禁忌





## 08.总结提炼

### 8.1 核心收获



### 8.2 知识图谱



### 8.3 延伸思考





## 09.参考资料

1. []()
2. []()
3. []()

